{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mltools as ml\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout, ZeroPadding1D, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.genfromtxt('data/X_train.txt', delimiter=None)\n",
    "Y = np.genfromtxt('data/Y_train.txt', delimiter=None)\n",
    "# X,Y = ml.shuffleData(X,Y)\n",
    "\n",
    "Xdf = pd.DataFrame(X)\n",
    "Ydf = pd.DataFrame(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>242.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>240.92</td>\n",
       "      <td>232.44</td>\n",
       "      <td>1195.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.26710</td>\n",
       "      <td>6.4128</td>\n",
       "      <td>1.98690</td>\n",
       "      <td>3.9756</td>\n",
       "      <td>2.3392</td>\n",
       "      <td>6.3537</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>249.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>242.31</td>\n",
       "      <td>233.68</td>\n",
       "      <td>1579.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.68310</td>\n",
       "      <td>6.0824</td>\n",
       "      <td>1.19640</td>\n",
       "      <td>3.4577</td>\n",
       "      <td>2.0416</td>\n",
       "      <td>7.6746</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>223.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>227.64</td>\n",
       "      <td>204.42</td>\n",
       "      <td>1034.0</td>\n",
       "      <td>603.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>1.52860</td>\n",
       "      <td>17.8690</td>\n",
       "      <td>13.23000</td>\n",
       "      <td>5.7120</td>\n",
       "      <td>4.7216</td>\n",
       "      <td>6.6030</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>234.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>236.27</td>\n",
       "      <td>229.73</td>\n",
       "      <td>7716.0</td>\n",
       "      <td>3907.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.60465</td>\n",
       "      <td>8.0497</td>\n",
       "      <td>3.44760</td>\n",
       "      <td>2.4845</td>\n",
       "      <td>1.5741</td>\n",
       "      <td>1.4205</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>234.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>245.51</td>\n",
       "      <td>234.10</td>\n",
       "      <td>545.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.74730</td>\n",
       "      <td>5.2649</td>\n",
       "      <td>0.86766</td>\n",
       "      <td>5.9626</td>\n",
       "      <td>4.2450</td>\n",
       "      <td>3.1429</td>\n",
       "      <td>24.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1       2       3       4       5      6        7        8   \\\n",
       "0  242.0  227.0  240.92  232.44  1195.0   253.0    0.0  1.26710   6.4128   \n",
       "1  249.0  230.0  242.31  233.68  1579.0   243.0    0.0  9.68310   6.0824   \n",
       "2  223.0  195.0  227.64  204.42  1034.0   603.0  318.0  1.52860  17.8690   \n",
       "3  234.0  221.0  236.27  229.73  7716.0  3907.0    0.0  0.60465   8.0497   \n",
       "4  234.0  233.0  245.51  234.10   545.0    21.0    0.0  6.74730   5.2649   \n",
       "\n",
       "         9       10      11      12    13  \n",
       "0   1.98690  3.9756  2.3392  6.3537   0.0  \n",
       "1   1.19640  3.4577  2.0416  7.6746   0.0  \n",
       "2  13.23000  5.7120  4.7216  6.6030   0.0  \n",
       "3   3.44760  2.4845  1.5741  1.4205   0.0  \n",
       "4   0.86766  5.9626  4.2450  3.1429  24.2  "
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Reducing Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_kBest = SelectKBest(f_classif, k=10).fit_transform(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>242.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>240.92</td>\n",
       "      <td>232.44</td>\n",
       "      <td>253.0</td>\n",
       "      <td>6.4128</td>\n",
       "      <td>1.98690</td>\n",
       "      <td>2.3392</td>\n",
       "      <td>6.3537</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>249.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>242.31</td>\n",
       "      <td>233.68</td>\n",
       "      <td>243.0</td>\n",
       "      <td>6.0824</td>\n",
       "      <td>1.19640</td>\n",
       "      <td>2.0416</td>\n",
       "      <td>7.6746</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>223.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>227.64</td>\n",
       "      <td>204.42</td>\n",
       "      <td>603.0</td>\n",
       "      <td>17.8690</td>\n",
       "      <td>13.23000</td>\n",
       "      <td>4.7216</td>\n",
       "      <td>6.6030</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>234.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>236.27</td>\n",
       "      <td>229.73</td>\n",
       "      <td>3907.0</td>\n",
       "      <td>8.0497</td>\n",
       "      <td>3.44760</td>\n",
       "      <td>1.5741</td>\n",
       "      <td>1.4205</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>234.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>245.51</td>\n",
       "      <td>234.10</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5.2649</td>\n",
       "      <td>0.86766</td>\n",
       "      <td>4.2450</td>\n",
       "      <td>3.1429</td>\n",
       "      <td>24.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1       2       3       4        5         6       7       8  \\\n",
       "0  242.0  227.0  240.92  232.44   253.0   6.4128   1.98690  2.3392  6.3537   \n",
       "1  249.0  230.0  242.31  233.68   243.0   6.0824   1.19640  2.0416  7.6746   \n",
       "2  223.0  195.0  227.64  204.42   603.0  17.8690  13.23000  4.7216  6.6030   \n",
       "3  234.0  221.0  236.27  229.73  3907.0   8.0497   3.44760  1.5741  1.4205   \n",
       "4  234.0  233.0  245.51  234.10    21.0   5.2649   0.86766  4.2450  3.1429   \n",
       "\n",
       "      9  \n",
       "0   0.0  \n",
       "1   0.0  \n",
       "2   0.0  \n",
       "3   0.0  \n",
       "4  24.2  "
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_kBestdf = pd.DataFrame(X_kBest)\n",
    "X_kBestdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Increasing Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 66)\n",
      "[  1.00000000e+00   2.42000000e+02   2.27000000e+02   2.40920000e+02\n",
      "   2.32440000e+02   2.53000000e+02   6.41280000e+00   1.98690000e+00\n",
      "   2.33920000e+00   6.35370000e+00   0.00000000e+00   5.85640000e+04\n",
      "   5.49340000e+04   5.83026400e+04   5.62504800e+04   6.12260000e+04\n",
      "   1.55189760e+03   4.80829800e+02   5.66086400e+02   1.53759540e+03\n",
      "   0.00000000e+00   5.15290000e+04   5.46888400e+04   5.27638800e+04\n",
      "   5.74310000e+04   1.45570560e+03   4.51026300e+02   5.30998400e+02\n",
      "   1.44228990e+03   0.00000000e+00   5.80424464e+04   5.59994448e+04\n",
      "   6.09527600e+04   1.54497178e+03   4.78683948e+02   5.63560064e+02\n",
      "   1.53073340e+03   0.00000000e+00   5.40283536e+04   5.88073200e+04\n",
      "   1.49059123e+03   4.61835036e+02   5.43723648e+02   1.47685403e+03\n",
      "   0.00000000e+00   6.40090000e+04   1.62243840e+03   5.02685700e+02\n",
      "   5.91817600e+02   1.60748610e+03   0.00000000e+00   4.11240038e+01\n",
      "   1.27415923e+01   1.50008218e+01   4.07450074e+01   0.00000000e+00\n",
      "   3.94777161e+00   4.64775648e+00   1.26241665e+01   0.00000000e+00\n",
      "   5.47185664e+00   1.48625750e+01   0.00000000e+00   4.03695037e+01\n",
      "   0.00000000e+00   0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "X_poly = PolynomialFeatures(2).fit_transform(X_kBest)\n",
    "print(X_poly.shape)\n",
    "print(X_poly[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_poly_train = X_poly[:180000,:]\n",
    "Y_poly_train = Y[:180000]\n",
    "X_poly_val = X_poly[180000:,:]\n",
    "Y_poly_val = Y[180000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180000, 14)\n",
      "(180000,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val = X[:180000,:], X[180000:,:]\n",
    "Y_train, Y_val = Y[:180000], Y[180000:]\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logReg = LogisticRegression()\n",
    "logReg.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68884999999999996"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logReg.score(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6223,)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = logReg.predict(X_val)\n",
    "wrong = Y_val[preds != Y_val]\n",
    "right = Y_val[preds == Y_val]\n",
    "wrong.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logReg = LogisticRegression()\n",
    "logReg.fit(X_poly_train, Y_poly_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68774999999999997"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logReg.score(X_poly_val, Y_poly_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logReg3 = LogisticRegression()\n",
    "logReg3.fit(X_poly_3rd_train, Y_poly_3rd_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68540000000000001"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logReg3.score(X_poly_3rd_val, Y_poly_3rd_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras - Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nnModel = Sequential()\n",
    "nnModel.add(BatchNormalization(input_shape=(66,)))\n",
    "nnModel.add(Dense(1024, activation=\"relu\"))\n",
    "nnModel.add(Dense(512, activation=\"relu\"))\n",
    "nnModel.add(Dense(512, activation=\"relu\"))\n",
    "nnModel.add(Dense(512, activation=\"relu\"))\n",
    "nnModel.add(Dense(512, activation=\"relu\"))\n",
    "nnModel.add(Dense(512, activation=\"relu\"))\n",
    "nnModel.add(Dense(512, activation=\"relu\"))\n",
    "nnModel.add(Dense(512, activation=\"relu\"))\n",
    "nnModel.add(Dense(1024, activation=\"relu\"))\n",
    "nnModel.add(Dense(1, activation='sigmoid'))\n",
    "adam = Adam(lr=.01)\n",
    "nnModel.compile(loss=\"binary_crossentropy\", optimizer=adam, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180000 samples, validate on 20000 samples\n",
      "Epoch 1/1\n",
      "180000/180000 [==============================] - 237s 1ms/step - loss: 5.5273 - acc: 0.6571 - val_loss: 5.5229 - val_acc: 0.6573\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a38d5f550>"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnModel.optimizer = Adam(lr=0.001)\n",
    "nnModel.fit(X_poly_train, Y_poly_train, epochs=1, batch_size=64, validation_data=(X_poly_val, Y_poly_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_37 (Batc (None, 66)                264       \n",
      "_________________________________________________________________\n",
      "dense_130 (Dense)            (None, 1024)              68608     \n",
      "_________________________________________________________________\n",
      "dense_131 (Dense)            (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_132 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_133 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_134 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_135 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_136 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_137 (Dense)            (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_138 (Dense)            (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dense_139 (Dense)            (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 2,695,945\n",
      "Trainable params: 2,695,813\n",
      "Non-trainable params: 132\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nnModel.summaryy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Network with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropout = .2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nnModel = Sequential()\n",
    "nnModel.add(BatchNormalization(input_shape=(66,)))\n",
    "nnModel.add(Dense(256, activation=\"relu\"))\n",
    "nnModel.add(BatchNormalization())\n",
    "nnModel.add(Dense(512, activation=\"relu\"))\n",
    "nnModel.add(Dropout(dropout))\n",
    "nnModel.add(Dense(512, activation=\"relu\"))\n",
    "nnModel.add(BatchNormalization())\n",
    "nnModel.add(Dense(1024, activation=\"relu\"))\n",
    "nnModel.add(BatchNormalization())\n",
    "nnModel.add(Dropout(dropout))\n",
    "nnModel.add(Dense(256, activation=\"relu\"))\n",
    "nnModel.add(BatchNormalization())\n",
    "nnModel.add(Dense(512, activation=\"relu\"))\n",
    "nnModel.add(BatchNormalization())\n",
    "nnModel.add(Dropout(dropout))\n",
    "nnModel.add(Dense(512, activation=\"relu\"))\n",
    "nnModel.add(BatchNormalization())\n",
    "nnModel.add(Dense(1024, activation=\"relu\"))\n",
    "nnModel.add(BatchNormalization())\n",
    "nnModel.add(Dropout(dropout))\n",
    "nnModel.add(Dense(256, activation=\"relu\"))\n",
    "nnModel.add(BatchNormalization())\n",
    "nnModel.add(Dense(512, activation=\"relu\"))\n",
    "nnModel.add(BatchNormalization())\n",
    "nnModel.add(Dropout(dropout))\n",
    "nnModel.add(Dense(512, activation=\"relu\"))\n",
    "nnModel.add(BatchNormalization())\n",
    "nnModel.add(Dense(1024, activation=\"relu\"))\n",
    "nnModel.add(BatchNormalization())\n",
    "nnModel.add(Dropout(dropout))\n",
    "nnModel.add(Dense(256, activation=\"relu\"))\n",
    "nnModel.add(BatchNormalization())\n",
    "nnModel.add(Dense(512, activation=\"relu\"))\n",
    "nnModel.add(BatchNormalization())\n",
    "nnModel.add(Dropout(dropout))\n",
    "nnModel.add(Dense(512, activation=\"relu\"))\n",
    "nnModel.add(BatchNormalization())\n",
    "nnModel.add(Dense(1024, activation=\"relu\"))\n",
    "nnModel.add(BatchNormalization())\n",
    "nnModel.add(Dropout(dropout))\n",
    "nnModel.add(Dense(256, activation=\"relu\"))\n",
    "nnModel.add(BatchNormalization())\n",
    "nnModel.add(Dense(512, activation=\"relu\"))\n",
    "nnModel.add(BatchNormalization())\n",
    "nnModel.add(Dropout(dropout))\n",
    "nnModel.add(Dense(512, activation=\"relu\"))\n",
    "nnModel.add(BatchNormalization())\n",
    "nnModel.add(Dense(1024, activation=\"relu\"))\n",
    "nnModel.add(BatchNormalization())\n",
    "nnModel.add(Dropout(dropout))\n",
    "nnModel.add(Dense(256, activation=\"relu\"))\n",
    "nnModel.add(BatchNormalization())\n",
    "nnModel.add(Dense(512, activation=\"relu\"))\n",
    "nnModel.add(BatchNormalization())\n",
    "nnModel.add(Dropout(dropout))\n",
    "nnModel.add(Dense(512, activation=\"relu\"))\n",
    "nnModel.add(BatchNormalization())\n",
    "nnModel.add(Dense(1024, activation=\"relu\"))\n",
    "\n",
    "nnModel.add(Dense(1, activation='sigmoid'))\n",
    "nnModel.compile(loss=\"binary_crossentropy\", optimizer=adam, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180000 samples, validate on 20000 samples\n",
      "Epoch 1/1\n",
      "180000/180000 [==============================] - 1646s 9ms/step - loss: 5.5305 - acc: 0.6568 - val_loss: 5.4914 - val_acc: 0.6593\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a55cb0350>"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnModel.optimizer = Adam(lr=0.01)\n",
    "nnModel.fit(X_poly_train, Y_poly_train, epochs=1, batch_size=16, validation_data=(X_poly_val, Y_poly_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Network with 3rd degree polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 286)\n"
     ]
    }
   ],
   "source": [
    "X_poly_3rd = PolynomialFeatures(3).fit_transform(X_kBest)\n",
    "print(X_poly_3rd.shape)\n",
    "X_poly_3rd_train = X_poly_3rd[:180000,:]\n",
    "Y_poly_3rd_train = Y[:180000]\n",
    "X_poly_3rd_val = X_poly_3rd[180000:,:]\n",
    "Y_poly_3rd_val = Y[180000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nnModel = Sequential()\n",
    "nnModel.add(BatchNormalization(input_shape=(286,)))\n",
    "nnModel.add(Dense(1024, activation=\"relu\"))\n",
    "nnModel.add(BatchNormalization())\n",
    "nnModel.add(Dense(1024, activation=\"relu\"))\n",
    "nnModel.add(Dropout(dropout))\n",
    "nnModel.add(Dense(2048, activation=\"relu\"))\n",
    "nnModel.add(BatchNormalization())\n",
    "nnModel.add(Dense(2048, activation=\"relu\"))\n",
    "nnModel.add(BatchNormalization())\n",
    "nnModel.add(Dense(1024, activation=\"relu\"))\n",
    "nnModel.add(BatchNormalization())\n",
    "nnModel.add(Dense(1024, activation=\"relu\"))\n",
    "nnModel.add(Dropout(dropout))\n",
    "nnModel.add(Dense(2048, activation=\"relu\"))\n",
    "nnModel.add(BatchNormalization())\n",
    "nnModel.add(Dense(2048, activation=\"relu\"))\n",
    "nnModel.add(BatchNormalization())\n",
    "nnModel.add(Dense(1024, activation=\"relu\"))\n",
    "nnModel.add(BatchNormalization())\n",
    "nnModel.add(Dense(1024, activation=\"relu\"))\n",
    "nnModel.add(Dropout(dropout))\n",
    "nnModel.add(Dense(2048, activation=\"relu\"))\n",
    "nnModel.add(BatchNormalization())\n",
    "nnModel.add(Dense(2048, activation=\"relu\"))\n",
    "nnModel.add(BatchNormalization())\n",
    "\n",
    "nnModel.add(Dense(512, activation=\"relu\"))\n",
    "nnModel.add(Dense(1, activation='sigmoid'))\n",
    "nnModel.compile(loss=\"binary_crossentropy\", optimizer=adam, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180000 samples, validate on 20000 samples\n",
      "Epoch 1/1\n",
      "180000/180000 [==============================] - 3391s 19ms/step - loss: 5.5576 - acc: 0.6550 - val_loss: 5.4914 - val_acc: 0.6593\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a42a061d0>"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnModel.optimizer = Adam(lr=0.01)\n",
    "nnModel.fit(X_poly_3rd_train, Y_poly_3rd_train, epochs=1, batch_size=32, validation_data=(X_poly_3rd_val, Y_poly_3rd_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nnModel.save_weights(\"deep_dense_weights.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "Conv1D?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(256, 3, padding=\"same\", activation='relu', input_shape=(286,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(256, 3, padding=\"same\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(2, padding=\"same\", strides=2))\n",
    "\n",
    "model.add(Conv1D(256, 3, padding=\"same\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(256, 3, padding=\"same\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(2, strides=2))\n",
    "\n",
    "model.add(Conv1D(512, 3, padding=\"same\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(512, 3, padding=\"same\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(2, padding=\"same\", strides=2))\n",
    "\n",
    "model.add(Conv1D(512, 3, padding=\"same\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(512, 3, padding=\"same\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(2, padding=\"same\", strides=2))\n",
    "\n",
    "model.add(Conv1D(256, 3, padding=\"same\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(256, 3, padding=\"same\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(2, strides=2))\n",
    "\n",
    "model.add(Conv1D(256, 3, padding=\"same\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(256, 3, padding=\"same\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(2, strides=2))\n",
    "\n",
    "model.add(Conv1D(128, 3, padding=\"same\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(128, 3, padding=\"same\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(2, padding=\"same\", strides=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(516, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_expanded = np.expand_dims(X_poly_3rd_train, axis=2)\n",
    "X_val_expanded = np.expand_dims(X_poly_3rd_val, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=adam, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180000 samples, validate on 20000 samples\n",
      "Epoch 1/1\n",
      " 40192/180000 [=====>........................] - ETA: 3:22:10 - loss: 0.7106 - acc: 0.6496"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-289-5d99a315ebb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m model.fit(X_train_expanded, Y_poly_3rd_train, epochs=1, \n\u001b[0;32m----> 3\u001b[0;31m           validation_data=(X_val_expanded, Y_poly_3rd_val), batch_size=32)\n\u001b[0m",
      "\u001b[0;32m/Users/johnlu/anaconda/envs/py2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Users/johnlu/anaconda/envs/py2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1710\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Users/johnlu/anaconda/envs/py2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/johnlu/anaconda/envs/py2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/johnlu/anaconda/envs/py2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/johnlu/anaconda/envs/py2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/johnlu/anaconda/envs/py2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/johnlu/anaconda/envs/py2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/johnlu/anaconda/envs/py2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.optimizer = Adam(lr=0.01)\n",
    "model.fit(X_train_expanded, Y_poly_3rd_train, epochs=1, \n",
    "          validation_data=(X_val_expanded, Y_poly_3rd_val), batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 14)\n",
      "(200000, 13)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "lsvc = LinearSVC(C=0.5, penalty=\"l1\", dual=False).fit(X, Y)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xBoostClf = Pipeline([('scaling', StandardScaler()),\n",
    "  ('classification', GradientBoostingClassifier())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {'max_depth': [1, 5, 9, 13, 17, 21, 23], 'n_estimators': [3, 7, 11, 15, 19, 23]},\n",
    "]\n",
    "xBoostClf = GradientBoostingClassifier()\n",
    "grid = GridSearchCV(xBoostClf, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'n_estimators': [3, 7, 11, 15, 19, 23], 'max_depth': [1, 5, 9, 13, 17, 21, 23]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xBoostCLF = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69084999999999996"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xBoostCLF.score(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {'max_depth': [1, 5, 9, 13, 17, 21, 23], 'n_estimators': [3, 7, 11, 15, 19, 23]},\n",
    "]\n",
    "randomForestClf = GradientBoostingClassifier()\n",
    "grid = GridSearchCV(randomForestClf, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'n_estimators': [3, 7, 11, 15, 19, 23], 'max_depth': [1, 5, 9, 13, 17, 21, 23]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=5,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=19,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "depths = [1, 5, 9, 13, 17, 21, 23]\n",
    "scores = []\n",
    "for depth in depths:\n",
    "    rfClf = RandomForestClassifier(n_estimators=10, max_depth=depth)\n",
    "    xValScores = cross_val_score(rfClf, X_new, Y)\n",
    "    scores.append(xValScores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6571050000328554, 0.6904250109595761, 0.7052950045602876, 0.7182350000859121, 0.7242300058112406, 0.7234000077112085]\n",
      "best depth: 17\n"
     ]
    }
   ],
   "source": [
    "print(scores)\n",
    "print(\"best depth: %d\" % depths[np.argmax(scores)])\n",
    "best_depth = depths[np.argmax(scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nEstimators = [3, 7, 11, 15, 19, 23]\n",
    "n_est_scores = []\n",
    "for n in nEstimators:\n",
    "    rfClf = RandomForestClassifier(n_estimators=n, max_depth=best_depth)\n",
    "    xValScores = cross_val_score(rfClf, X_new, Y)\n",
    "    n_est_scores.append(xValScores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6571050000328554, 0.6904250109595761, 0.7052950045602876, 0.7182350000859121, 0.7242300058112406, 0.7234000077112085]\n",
      "best n estimators: 19\n"
     ]
    }
   ],
   "source": [
    "print(n_est_scores)\n",
    "print(\"best n estimators: %d\" % nEstimators[np.argmax(n_est_scores)])\n",
    "best_n_est = nEstimators[np.argmax(n_est_scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfClf = RandomForestClassifier(n_estimators=best_n_est, max_depth=best_depth)\n",
    "cross_val_score(rfClf, X_new, Y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_samples_sp = [2, 7, 12, 17, 22, 27]\n",
    "min_samples_scores = []\n",
    "for m in min_samples_sp:\n",
    "    rfClf = RandomForestClassifier(n_estimators=n, max_depth=best_depth, min_samples_split=m)\n",
    "    xValScores = cross_val_score(rfClf, X_new, Y)\n",
    "    min_samples_scores.append(xValScores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7301700072365448, 0.728255001061418, 0.73007000331152, 0.7290600059614828, 0.7277900076614278, 0.7275100031363912]\n",
      "best min_samples: 2\n"
     ]
    }
   ],
   "source": [
    "print(min_samples_scores)\n",
    "print(\"best min_samples: %d\" % min_samples_sp[np.argmax(min_samples_scores)])\n",
    "best_min_samples = min_samples_sp[np.argmax(min_samples_scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rlClf = Pipeline([('scaling', StandardScaler()),\n",
    "  ('classification', RandomForestClassifier(n_estimators=best_n_est, max_depth=best_depth, min_samples_split=best_min_samples))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ada Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adaClf = Pipeline([('scaling', StandardScaler()),\n",
    "  ('classification', AdaBoostClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68743156, 0.69256154, 0.6897669 ])"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(clf, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_poly_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xBoostClf = Pipeline([('scaling', StandardScaler()),\n",
    "  ('classification', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "rfClf = Pipeline([('scaling', StandardScaler()),\n",
    "  ('classification', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "adaClf = Pipeline([('scaling', StandardScaler()),\n",
    "  ('classification', AdaBoostClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adaClf.fit(X_poly_train, Y_poly_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69758888888888892"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaClf.score(X_poly_train, Y_poly_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaling', StandardScaler(copy=True, with_mean=True, with_std=True)), ('classification', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split...n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfClf.fit(X_train, Y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67815000000000003"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfClf.score(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaling', StandardScaler(copy=True, with_mean=True, with_std=True)), ('classification', GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impu...      presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False))])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xBoostClf.fit(X_train, Y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69494999999999996"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xBoostClf.score(X_val, Y_val);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=19, max_depth=5).fit(X_train, Y_train)\n",
    "ada = AdaBoostClassifier(n_estimators=19).fit(X_train, Y_train)\n",
    "xb = GradientBoostingClassifier(n_estimators=19, max_depth=5).fit(X_train, Y_train)\n",
    "models = [rf, ada, xb]\n",
    "predictions = []\n",
    "for x in X_val:\n",
    "    preds = [0,0]\n",
    "    for model in models:\n",
    "        pred = model.predict(x.reshape(1,14)).astype(int)\n",
    "        preds[pred[0]] = preds[pred[0]] + 1\n",
    "    predictions.append(np.argmax(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68855\n"
     ]
    }
   ],
   "source": [
    "acc = (predictions == Y_val).sum().astype(float) / Y_val.shape[0]\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3, 3, 3, 3, 2, 3])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py2)",
   "language": "python",
   "name": "py2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
